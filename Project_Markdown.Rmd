---
output: pdf_document
header-includes:
  - \usepackage{xcolor}
  - \everymath{\color{black}}
  - \everydisplay{\color{black}}
  - \setlength{\parskip}{0.1em}
  - \usepackage{wrapfig}
fontsize: 10pt             # Options: 10pt, 11pt, 12pt
geometry: margin=1in       # Sets all margins to 1 inch
linestretch: 1           # 1.5 line spacing
---

\begin{center}
\Large \textbf{Assignment 7}
\end{center}
\vspace{-0.1cm}

# 1st Task: Reproduce Figure 6.20

### 1.1 Setup and Data

```{r setup_and_data, message=FALSE, warning=FALSE}
setwd("~/Project_Solution")
library(pls)
Credit <- read.csv("../Project_Solution/Credit.csv")
set.seed(1) # Set seed for reproducibility of PCR 
```

### 1.2 Run Principal Component Regression

PCA turns variables that use different scales and are correlated into a smaller set of new variables that all use the same scale, are uncorrelated.
PCR then predicts Balance using these standardized super-variables, and cross-validation assesses model fit.:
```{r run_pcr}
pcr_model <- pcr(
  # standardize the data to a z-Distribution (scale = TRUE)
  Balance ~ ., data = Credit, scale = TRUE, validation = "CV")
```

Normalising is done by transforming each variable X into a z-value, which means subtracting its mean and dividing by its standard deviation: $$
Z_m = \sum_{j=1}^{p} \phi_{jm} X_j,
\qquad
\text{with } \sum_{j=1}^{p} \phi_{jm}^2 = 1.
$$

PCR Regression Model $$
Y_i = \theta_0 + \sum_{m=1}^{M} \theta_m Z_{im} + \varepsilon_i.
$$

Relationship between original coefficients and PCR coefficients $$
\beta_j = \sum_{m=1}^{M} \theta_m \phi_{jm}
$$

### 1.3 Extract Coefficients

The raw coefficients from a 3D array: `[Variables, Balance, PCs]`. We select "Balance" because it's our dependent variable. For each category (e.g. Income), we take the coefficient that explains the relationship between that variable and Balance.

```{r extract_coefficients}
coef_matrix <- pcr_model$coefficients[, "Balance", ]
```

### 1.4 Data Preparation for Smoothing

In Principal Component Analysis (PCA), we cannot create more components than we have input variables. Component 1 explains the most variance, while Component 11 explains the least. If we use all 11 components, our PCR model becomes exactly the same as a standard OLS.

```{r data_prep_smoothing}
n_components <- 11
stretch_factor <- 10 
# turns 11 data points into 110 so the plot is more beautiful (and like the textbook)
```

The stretch factor means we repeat each coefficient column several times to smoothen the plotting grid. We then transpose ('t') so that components appear on the x-axis and variables on the y-axis in the matplot.

```{r stretch_coefficients}
stretched_coefs <- t(coef_matrix[, rep(1:n_components, each = stretch_factor)])
```

Create the x-axis grid.

```{r Set_Grid}
x_grid <- seq(1, n_components, length = n_components * stretch_factor)
```

### 1.5 Plotting

```{r plotting,fig.width=8, fig.height=4}
par(mfrow = c(1, 2))
par(cex.axis = 0.7)
head(stretched_coefs,0)
line_cols <- c("black", "red", "blue", "grey", "grey", "grey", "grey", 
               "orange","grey", "grey", "grey")
line_types <- c(1, 2, 3, 1, 1, 1, 1, 4, 1, 1, 1)
line_widths <- c(2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1)

# --- Plot Left: Standardized Coefficients ---
matplot(x_grid, stretched_coefs, type = 'l', col = line_cols, lty = line_types, 
        lwd = line_widths, xlab = "Number of Components", 
        ylab = "Standardized Coefficients", cex.lab = 0.8,
        main="Plot 6.20", cex.main=1)
legend("topleft", names(Credit)[c(1, 2, 3, 8)], col = c("black", "red", "blue", "orange"), 
       lty = 1:4, lwd = 1, bty = "n", cex = 0.6)

# --- Plot Right: Cross-Validation MSE ---
# Using 'MSEP' function to extract Mean Squared Error of Prediction
cv_mse <- MSEP(pcr_model)$val[1, 1, -1] 
# $val$ (1-> Cross Validation Standard error, 1 -> Balance, -1 -> Remove Intercept )
plot(cv_mse, 
     col = "purple", type = "b", ylab = "Cross-Validation MSE", xlab = "Number of Components", 
     cex.lab = 0.8)
```

# 2nd Task: Ridge Regression & Shrinkage

### 2.1 Setup

Problem - ISLR (p. 239): As $\lambda$ increases, the $\ell_2$-norm of $\hat{\beta}_\lambda$ decreases, and so does $\frac{\|\hat{\beta}^R_\lambda\|_2}{\|\hat{\beta}^{\mathrm{OLS}}\|_2}$."

Goal:

1.  Provide a clean mathematical explanation using the Singular Value Decomposition of $X$
2.  Prove monotone shrinkage of $\hat{\beta}^R_\lambda\|2$, explain why the shrinkage ratio tends to decrease from 1 to 0.

**Setup and Definitions:**

***Data:***

$$ y \in \mathbb{R}^n, \qquad X \in \mathbb{R}^{n \times p} \quad \text{(standardized columns, centered y)} $$

***L2-norm:*** $$ \|\beta\|_2 = \sqrt{\sum_{j=1}^p \beta_j^2} \quad \quad \quad (Formula \quad 2.1)$$

***Shrinkage ratio (ISLR x-axis):***

$$ \text{Shrinkage}(\lambda) = \frac{\|\hat{\beta}^R_\lambda\|_2}{\|\hat{\beta}^{\mathrm{OLS}}\|_2} \quad \quad \quad (Formula \quad 2.2)$$

## 2.2. Explanation of Ridge Regression Norms

Definition of Lambda ($\lambda$): $\lambda$ is the tuning parameter defined in shrinkage regression (Ridge), which serves to artificially minimize (shrink) the coefficients ($\beta$) Ridge Regression Formula, the general Ridge regression objective is:

$$RSS + \lambda\sum_{j=1}^{p}\beta_{j}^{2} \quad \quad \quad (Formula \quad 2.3)$$

$$RSS = \sum_{i=1}^{n}\left(y_{i}-\sum_{j=1}^{p}x_{ij}\beta_{j}\right)^{2} \quad \quad \quad (Formula \quad 2.4)$$

### 2.3 ISLR Simplification (Special Case)

In the special case presented in ISLR (Equation 6.14), we assume no intercept (centered data), $N=p$, and the design matrix $X$ is a diagonal matrix with 1s on the diagonal. This effectively means we have orthogonal predictors with unit length. Meaning in this simplified example, we assume the variables are completely separate and don't interfere with each other and they are all scaled to have the exact same strength, which basically means we can calculate each answer individually. The expanded least squares term is:

$$\sum_{i=1}^{n}\left(y_{i}-\sum_{j=1}^{p}x_{ij}\beta_{j}\right)^{2}+\lambda\sum_{j=1}^{p}\beta_{j}^{2} \quad \quad \quad (Formula \quad 2.5)$$

$y$: The target vector (response variable) containing the observed values.

$x_j$: The vector representing the $j$-th predictor (feature column)

$\beta_j$: The regression coefficient (weight) for the $j$-th predictor

$\lambda$: The tuning parameter (shrinkage constant) that controls the penalty strength

$j, k$: Indices used as counters to iterate through the predictors in the summation

$\top$: The transpose symbol, indicating the vector is flipped from column to row for multiplication.

In matrix notation, expanding the square gives:

$$y^{\top}y - 2\sum_{j}\beta_{j}x_{j}^{\top}y + \sum_{j}\sum_{k}\beta_{j}\beta_{k}x_{j}^{\top}x_{k} + \lambda\sum_{j}\beta_{j}^{2} \quad \quad \quad (Formula \quad 2.6)$$

**Implications of Orthogonality and Unit Length:** Because $X$ is orthogonal ($x_{j}^{\top}x_{k} = 0$ for $j \neq k$) and has unit length ($x_{j}^{\top}x_{j} = 1$), the cross-terms cancel out

$$x_{j}^{\top}x_{k} = \begin{cases} 0 & \text{if } j \neq k \\ 1 & \text{if } j = k \end{cases}$$

This simplifies the interaction term in Formula 2.6: $$\sum_{j,k}\beta_{j}\beta_{k}x_{j}^{\top}x_{k} = \sum_{j=1}^{p}\beta_{j}^{2} \quad \quad \quad (Formula \quad 2.7)$$

This then yields, when Substituting the interaction term by the simplified version:

$$y^{\top}y - 2\sum_{j}\beta_{j}x_{j}^{\top}y + \sum_{j=1}^{p}\beta_{j}^{2} + \lambda\sum_{j}\beta_{j}^{2} \quad \quad \quad (Formula \quad 2.8)$$

Consequently, the optimization problem separates into $p$ independent univariate problems. The Ridge estimator becomes: $$\hat{\beta}_{j,\lambda}^{R} = \frac{y_{j}}{1+\lambda} \quad \quad \quad (Formula \quad 2.9)$$

### 2.4 Minimization Derivation

To find the minimum, we differentiate the simplified objective function with respect to $\beta_j$. Starting from the expanded matrix form assuming orthogonal unit vectors where $x_j^{\top}y = y_j$ and $y^{\top}y = \sum y_j^2$:

$$
\begin{aligned}
\mathcal{L} &= y^{\top}y - 2\sum_{j=1}^{p}\beta_{j}x_{j}^{\top}y + \sum_{j=1}^{p}\beta_{j}^{2} + \lambda\sum_{j=1}^{p}\beta_{j}^{2} \quad \quad \quad (Formula \quad 2.10)\\
&= \sum_{j=1}^{p} y_j^2 - 2\sum_{j=1}^{p}\beta_{j}y_{j} + \sum_{j=1}^{p}\beta_{j}^{2} + \lambda\sum_{j=1}^{p}\beta_{j}^{2} && \text{(Substitute scalar equivalents)} \\
&= \sum_{j=1}^{p} \left( y_j^2 - 2\beta_{j}y_{j} + \beta_{j}^{2} \right) + \lambda\sum_{j=1}^{p}\beta_{j}^{2} && \text{(Group terms by index } j) \\
&= \sum_{j=1}^{p} (y_j - \beta_j)^2 + \sum_{j=1}^{p} \lambda \beta_j^2 && \text{(Complete the square)} \\
&= \sum_{j=1}^{p} \left[ (y_j - \beta_j)^2 + \lambda \beta_j^2 \right] && \text{(Final Sum of Independent Problems)}
\end{aligned}
$$

To find the minimum, we differentiate the term inside the brackets with respect to $\beta_j$: $$ \frac{\partial\mathcal{L}}{\partial \beta_j} \left( (y_j - \beta_j)^2 + \lambda \beta_j^2 \right) = -2(y_j - \beta_j) + 2\lambda\beta_j = 0 $$ $$\frac{\partial\mathcal{L}}{\partial\beta_{j}} = -2(y_{j}-\beta_{j}) + 2\lambda\beta_{j} = 0$$ $$-2y_{j} + 2(1+\lambda)\beta_{j} = 0$$

Rearranging for $\beta_j$ yields the Ridge estimator for this special case: $$\hat{\beta}_{j,\lambda}^{R} = \frac{y_{j}}{1+\lambda} \quad \quad \quad (Formula \quad 2.11) $$

```{r plotting_norm_vs_lambda, fig.width=8, fig.height=2.5, fig.env='wrapfigure', fig.pos='l', out.width='100%'}
X <- scale(model.matrix(Balance~.,Credit)[,-1]); y <- scale(Credit$Balance)[,1]
XtX <- t(X)%*%X; Xty <- t(X)%*%y; L <- exp(seq(-2,10,len=100))
B <- sapply(L, function(l) solve(XtX+diag(l,ncol(X)), Xty))
rat <- apply(B, 2, function(b) sqrt(sum(b^2))) / sqrt(sum(solve(XtX,Xty)^2))
par(mar = c(5.1, 6.5, 4.1, 2.1))
plot(L, rat, type="l", log="x", col="darkblue", lwd=1.5,
     xlab = expression(lambda), ylab = "", # Suppress default label
     main = "Shrinkage Profile",las = 1)
title(ylab = expression(frac("||" * hat(beta)[lambda]^R * "||"[2], 
                             "||" * hat(beta)^OLS * "||"[2])), line = 2.1, cex.lab = 0.6)
grid()
```

### 2.5 Results

Norm Relationship From the formula above, we can derive the relationship between the norms

Vector Form: Stacking the coefficients into a vector: $$\hat{\beta}_{\lambda}^{R} = \frac{1}{1+\lambda}* y$$

Squared $l_2$ Norm: Taking the squared $l_2$ norm of both sides: $$||\hat{\beta}_{\lambda}^{R}||_{2}^{2} = \frac{1}{(1+\lambda)^{2}}\sum_{j=1}^{p}y_{j}^{2}$$ $$||\hat{\beta}_{\lambda}^{R}||_{2}^{2} = \frac{1}{(1+\lambda)^{2}}||y||_{2}^{2}$$

Final $l_2$ Norm: Taking the square root uses the standard identity $||cv||_{2} = |c| \cdot ||v||_{2}$: $$||\hat{\beta}_{\lambda}^{R}||_{2} = \frac{1}{1+\lambda}||y||_{2}$$

This mathematical derivation confirms the statement: as $\lambda$ increases, the denominator $(1+\lambda)$ increases, causing the norm $||\hat{\beta}_{\lambda}^{R}||_{2}$ to decrease.
